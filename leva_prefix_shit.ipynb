{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pylev\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm_notebook\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open ('norm_words.txt') as f:\n",
    "    norm_words = f.read().split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = 'C:\\\\Users\\\\1\\\\Desktop\\\\Kidorashi\\\\master\\\\texts\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file = open('prefix_with_gi.csv', encoding = 'UTF-8').read()\n",
    "prefs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('norm_words.txt', encoding = 'UTF-8') as f:\n",
    "    tokenized_texts_flat = f.read().split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = open('dictionary.html', encoding = \"UTF-8\")\n",
    "dictionary = BeautifulSoup(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dict = []\n",
    "for txt in dictionary.find_all('strong'):\n",
    "    tokenized_dict.append(txt.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3840"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenized_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "f = re.split('\\n', file)\n",
    "prefs = []\n",
    "for i in f:\n",
    "    n = re.sub(',', ' ', i)\n",
    "    n = re.sub('  ', ' ', n)\n",
    "    if n != '':\n",
    "        prefs.append(' '.join(n.rsplit()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_dict = []\n",
    "clear_dict_dict = defaultdict()\n",
    "for i, lemma_raw in enumerate(set(tokenized_dict)):\n",
    "    if any(lemma_raw for i in prefs) == False:\n",
    "        clear_dict_dict[lemma_raw] = lemma_raw\n",
    "    else:\n",
    "        for l in prefs:\n",
    "            if l in lemma_raw:\n",
    "                lemma = lemma_raw.strip(l)\n",
    "    #             clear_dict.append(lemma)\n",
    "                clear_dict_dict[lemma] = lemma_raw\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "any('heramuoter' for i in prefs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_tokenized_texts_flat = []\n",
    "clear_tokenized_texts_dict = defaultdict()\n",
    "for i, lemma_raw in enumerate(set(tokenized_texts_flat)):\n",
    "    if any(lemma_raw for i in prefs) == False:\n",
    "        clear_tokenized_texts_dict[lemma_raw] = lemma_raw\n",
    "    else:\n",
    "        for l in prefs:\n",
    "            if l in lemma_raw:\n",
    "                lemma = lemma_raw.strip(l)\n",
    "                clear_tokenized_texts_dict[lemma] = lemma_raw\n",
    "                break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clusters_fu = defaultdict(list)\n",
    "list_tokenized_texts_flat = list(set(tokenized_texts_flat))\n",
    "list_tokenized_dict = list(set(tokenized_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f42d98f60b14e8895142691a5621682",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for word in tqdm_notebook(list_tokenized_texts_flat):\n",
    "    for i, lemma in enumerate(list_tokenized_dict):\n",
    "#         dist = distance.get_jaro_distance(word, lemma, winkler=False)\n",
    "        dist = pylev.levenshtein(word, lemma)\n",
    "        if len(word) < 4 and len(lemma) <4:\n",
    "            if dist < 2:\n",
    "        #             word_dictionary = clear_dict_dict[]\n",
    "                clusters_fu[list_tokenized_dict[i]].append(word)\n",
    "        else:\n",
    "            if dist < 4:\n",
    "        #             word_dictionary = clear_dict_dict[]\n",
    "                clusters_fu[list_tokenized_dict[i]].append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "print(pylev.levenshtein('fundauit', 'funf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'manago' in norm_words"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "widgets": {
   "state": {
    "cf508759621d442aaa0ae05c7e269dc4": {
     "views": [
      {
       "cell_index": 21
      }
     ]
    },
    "e3f52d48a25f4e45b81594d856946110": {
     "views": [
      {
       "cell_index": 26
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
